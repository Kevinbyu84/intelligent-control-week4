Analisis Hasil:
•	Bagaimana performa agen dalam mengontrol environment CartPole?
Agen RL belajar menyeimbangkan tiang pada CartPole melalui banyak percobaan. Awalnya, sering gagal, namun seiring waktu agen menjadi lebih baik dan mampu menyeimbangkan tiang lebih lama.
•	Bagaimana perubahan parameter (misal: gamma, epsilon, learning rate) mempengaruhi kinerja agen?
- Gamma (Discount Factor) Semakin tinggi nilai gamma, agen mempertimbangkan reward jangka panjang. Jika gamma terlalu rendah, agen hanya fokus pada reward langsung. 
- Epsilon (Exploration Rate) Nilai epsilon tinggi membuat agen mencoba berbagai aksi secara acak untuk menemukan strategi terbaik. Secara bertahap, nilai epsilon diturunkan agar agen lebih sering memilih aksi yang terbukti efektif. 
- Learning Rate menentukan seberapa cepat agen mengubah perkiraan nilainya. Nilai yang terlalu tinggi bisa membuat pembelajaran tidak stabil, sedangkan nilai yang terlalu rendah membuat proses belajar menjadi lambat.
•	Apa tantangan yang muncul selama pelatihan agen RL?
Agen kadang mengalami fluktuasi reward yang menyebabkan pembelajaran tidak stabil. Menentukan kapan harus mencoba aksi baru (eksplorasi) dan kapan harus menggunakan aksi yang sudah diketahui efektif (eksploitasi) merupakan tantangan tersendiri. Proses pelatihan, terutama dengan algoritma seperti DQN, membutuhkan banyak waktu dan sumber daya komputasi.
Diskusi:
•	Perbedaan Utama antara Reinforcement Learning dan Supervised Learning:
Supervised learning menggunakan data yang sudah dilabeli, sedangkan RL belajar dari interaksi langsung dengan lingkungan tanpa label. Dalam supervised learning, model belajar dari contoh yang benar. Di RL, agen belajar melalui trial-and-error dengan mendapatkan reward atau hukuman. RL berfokus pada mendapatkan reward kumulatif maksimal, sedangkan supervised learning fokus pada akurasi prediksi.
•	Optimasi Strategi Eksplorasi dan Eksploitasi:
Eksplorasi, Agen mencoba berbagai aksi untuk menemukan strategi terbaik.
Eksploitasi, Agen menggunakan strategi yang sudah terbukti memberikan reward tinggi.
Optimalisasi, Pengurangan nilai epsilon secara bertahap (epsilon decay) membantu menyeimbangkan antara eksplorasi (mencoba hal baru) dan eksploitasi (menggunakan strategi yang sudah diketahui efektif).
•	Potensi Aplikasi RL dalam Sistem Kendali Nyata:
Robotika, Untuk mengontrol pergerakan robot dalam navigasi atau manipulasi objek.
Kendaraan Otonom, Mengoptimalkan keputusan pada mobil self-driving.
